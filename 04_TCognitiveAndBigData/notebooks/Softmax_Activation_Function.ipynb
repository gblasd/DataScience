{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOBOq6JKM41bFx1ansaDWU1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gblasd/DataScience/blob/BDCC/04_TCognitiveAndBigData/notebooks/Softmax_Activation_Function.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Softmax Activation Function"
      ],
      "metadata": {
        "id": "21JBBzD8hAPD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Activation function__ are the bakcbone of neuronal networks. Softmax is better at handling situations where outputs be interpreted as probabilities across mutually exclusive classes.\n",
        "\n",
        "The softmax activation function transforms an entire vector of numbes into a probability distribution. This unique characteristic makes it indipensable for tasks where we need to classify inputs into one of several possible catefories.\n",
        "\n",
        "The Softmax activation function is a mathematical function that transforms a vector of raw model outputs, knon as logits, into a probability distribution. In simple termsm it takes a set of numbers and converts into probabilities that sum up to 1.\n",
        "\n",
        "In neuronal networks, softmax is applies to the final layer of a network designed for multi-class classification. The raw outputs from a neuronal network's final layer are often called \"logits\". These values can range from negative infinity to positive infinity and don't have a direct probabilistic interpretation.\n",
        "\n",
        "The softmax activation function transforms these logits into a more interpretable from by:\n",
        "\n",
        "1.- Taking the exponential of each input value (wich ensures all values are positive).\n",
        "2.- Dividing each exponential by the sum of all exponentials (which ensures the outputs sum to 1).\n",
        "\n",
        "This transformation is crucial beacause it allows us to interpret the network's output as a probability distribution. For example, if a neuronal network is classifiying images into three categories `(cat, dog, bird)`, the softmax output might be `[0.7, 0.2, 0.1]`, indicating a `70%` probability for cat, 20% for dog, and 10% for bird.\n",
        "\n",
        "The softmax activation function plays a vital role in creating valid probability distributions because:\n",
        "\n",
        "- It ensures all output vaues are between 0 and 1.\n",
        "- It guarantees that all outputs sum to exactly 1.\n",
        "- it preserves the ranking of input values (larger inputs result in larger probabilities).\n",
        "- It accentuates differences between larger values while suppressing differences between smaller values.\n",
        "\n",
        "\n",
        "The softmax activation function formula can be expressed mathematically as:\n",
        "\n",
        "$$\\sigma (z_1) = \\frac{e^(z_i)}{\\sum_(j=1)^K e^(z_i)} $$\n",
        "\n",
        "Where:\n",
        "\n",
        "- $z_i$ is the input value (logit) for class $i$.\n",
        "- $e^(z_i)$ is the exponential function applied to $z_i$ (also written as exp($z_i$)).\n",
        "- $K$ is the total number of classes.\n",
        "- The denominator sums the exponentials of all inputs, ensuring the output values sum to 1.\n",
        "\n",
        "The softmax activation functions follows these steps to tranform a vector of inputs into a probability distribution:\n",
        "\n",
        "1.- __Calculate exponentials__: First, we compute the exponential ($e^x$) of each input value. This step ensures all values are positive, as $e^x > 0$ for any real number $x$.\n",
        "2.- __Sum the exponentials__: We calculate the sum of all the exponential values from step 1.\n",
        "3.- __Normalize__: We divide each exponential values by the sum calculated in step 2. This normalization ensures that all outputs are between $0$ and $1$ and sum to exactly $1$.\n",
        "\n",
        "__Practice__\n",
        "\n",
        "Suppose we have a neuronal network with three output neurons for a three-class classification problem. After the final computation, the network outputs the following logits: $z=[2.0, 1.0, 0.5]$.\n",
        "\n",
        "To convert these logits into probabilities using the softmax function:\n",
        "\n",
        "1.- __Calculate exponentials:__\n",
        "\n",
        "* $e^(2.0)=7.389$\n",
        "* $e^(1.0)=2.718$\n",
        "* $e^(0.5)=1.648$\n",
        "\n",
        "2.- __Sum the exponentials:__\n",
        "* $7.389+2.718+1.648=11.755$\n",
        "\n",
        "3.- __Normalize:__\n",
        "* $P(class 1)=\\frac{7.389}{11.755} \\sim 0.628 \\text{ or } 62.8\\%$\n",
        "* $P(class 2)=\\frac{2.718}{11.755} \\sim 0.231 \\text{ or } 23.1\\%$\n",
        "* $P(class 3)=\\frac{1.648}{11.755} \\sim 0.140 \\text{ or } 14.0\\%$\n",
        "\n",
        "\n",
        "The resulting _probabilty distribution_ [0.628, 0.231, 0.140] sum to 1, with the highest probabilty assigned to the class corresponding to the highest logit value.\n"
      ],
      "metadata": {
        "id": "b6XT5p0qhHm3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "q0NFYvSZg8Gm"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def softmax(x):\n",
        "    \"\"\"\n",
        "    Compute softmax values for each set of scores in x.\n",
        "\n",
        "    Args:\n",
        "        x: Input array of shape (batch_size, num_classes) or (num_classes,)\n",
        "\n",
        "    Returns:\n",
        "        Softmax probabilties of same shape as input\n",
        "    \"\"\"\n",
        "\n",
        "    # For numerical stability, subtract the maximum values from each input vector\n",
        "    # This prevents overflow when calculating exp(x)\n",
        "    shifted_x = x - np.max(x, axis=1, keepdims=True)\n",
        "\n",
        "    # Calculate exp(x) for each element\n",
        "    exp_x = np.exp(shifted_x)\n",
        "\n",
        "    # Calculate the sum of exp(x) for normalization\n",
        "    sum_exp_x = np.sum(exp_x, axis=1, keepdims=True)\n",
        "\n",
        "    # Normalize to get probabilities\n",
        "    probabilities = exp_x / sum_exp_x\n",
        "\n",
        "    return probabilities"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample logits from a neural network (batch of 2 examples, 3 classes each)\n",
        "logits = np.array([\n",
        "    [2.0, 1.0, 0.5],  # First example\n",
        "    [3.0, 2.0, 1.0]   # Second example\n",
        "])\n",
        "\n",
        "probabilities = softmax(logits)\n",
        "print(\"Logits:\\n\", logits)\n",
        "print(\"\\nSoftmax probabilities:\\n\", probabilities)\n",
        "print(\"\\nSum of probabilities (should be 1 for each example):\", np.sum(probabilities, axis=1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v9i6u8h8peFU",
        "outputId": "6576f4c3-5eec-494e-b7eb-b60f81079c51"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logits:\n",
            " [[2.  1.  0.5]\n",
            " [3.  2.  1. ]]\n",
            "\n",
            "Softmax probabilities:\n",
            " [[0.62853172 0.2312239  0.14024438]\n",
            " [0.66524096 0.24472847 0.09003057]]\n",
            "\n",
            "Sum of probabilities (should be 1 for each example): [1. 1.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Softmax\n",
        "from tensorflow.keras.datasets import mnist\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Load and preprocess the MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Normalize pixel values to be between 0 and 1\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "# One-hot encode the labels\n",
        "y_train_one_hot = tf.keras.utils.to_categorical(y_train, 10)\n",
        "y_test_one_hot = tf.keras.utils.to_categorical(y_test, 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Luqquiseplkp",
        "outputId": "5b2e3821-2bec-4f94-d3f8-e0d7ceb9f822"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we demonstrate two equivalent ways to incorporate the softmax activation function in a neural network:\n",
        "\n",
        "- __Method 1__: Directly specifying 'softmax' as the activation function in the final Dense layer.\n",
        "- __Method 2__: Adding a separate Softmax layer after the Dense layer that outputs raw logits.\n",
        "\n",
        "Both approaches produce identical results, but the second method makes the separation between logits and probabilities more explicit, which can be useful in certain scenarios."
      ],
      "metadata": {
        "id": "3HEaFPIHqBtB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Method 1: Using softmax as the activation function in the final layer\n",
        "model1 = Sequential([\n",
        "    Flatten(input_shape=(28, 28)),  # Convert 28x28 images to 784-length vectors\n",
        "    Dense(128, activation='relu'),   # Hidden layer with ReLU activation\n",
        "    Dense(10, activation='softmax')  # Output layer with softmax activation\n",
        "])\n",
        "\n",
        "# Method 2: Using a separate Softmax layer\n",
        "model2 = Sequential([\n",
        "    Flatten(input_shape=(28, 28)),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(10),  # Linear output (logits)\n",
        "    Softmax()   # Separate softmax layer\n",
        "])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZEwjNw_pomg",
        "outputId": "2871fbeb-260b-4276-f0f1-175a752613b6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We compile the model using the categorical cross-entropy loss function, which is designed to work with softmax outputs. This loss function measures the difference between the predicted probability distribution and the true one-hot encoded labels. We use the Adam optimizer, which adaptively adjusts the learning rate during training."
      ],
      "metadata": {
        "id": "tr_HiEUwqOgM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model1.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',  # This loss works well with softmax\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Print model summary\n",
        "print(\"Model with softmax activation:\")\n",
        "model1.summary()\n",
        "\n",
        "# Train the model\n",
        "print(\"\\nTraining the model...\")\n",
        "history = model1.fit(\n",
        "    x_train, y_train_one_hot,\n",
        "    epochs=5,\n",
        "    batch_size=128,\n",
        "    validation_split=0.1,\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "id": "oQvQmzQOprv2",
        "outputId": "282a5e6c-13a8-42f7-8511-2c99345a36cb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model with softmax activation:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m100,480\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m1,290\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">100,480</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,290</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m101,770\u001b[0m (397.54 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">101,770</span> (397.54 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m101,770\u001b[0m (397.54 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">101,770</span> (397.54 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training the model...\n",
            "Epoch 1/5\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - accuracy: 0.8297 - loss: 0.6359 - val_accuracy: 0.9537 - val_loss: 0.1733\n",
            "Epoch 2/5\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9461 - loss: 0.1902 - val_accuracy: 0.9665 - val_loss: 0.1278\n",
            "Epoch 3/5\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9614 - loss: 0.1342 - val_accuracy: 0.9710 - val_loss: 0.1029\n",
            "Epoch 4/5\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9709 - loss: 0.1015 - val_accuracy: 0.9748 - val_loss: 0.0891\n",
            "Epoch 5/5\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9773 - loss: 0.0796 - val_accuracy: 0.9765 - val_loss: 0.0857\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code selects a test image, passes it through the trained model, and visualizes both the image and the resulting probability distribution from the softmax output. The highest bar in the probability plot represents the model's prediction.\n",
        "\n"
      ],
      "metadata": {
        "id": "olVM3GQGqRLa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's check predictions on a sample\n",
        "sample_idx = 42\n",
        "sample_image = x_test[sample_idx]\n",
        "true_label = y_test[sample_idx]\n",
        "\n",
        "# Get model predictions (probabilities across all classes)\n",
        "predictions = model1.predict(sample_image[np.newaxis, ...])\n",
        "\n",
        "# Visualize the results\n",
        "plt.figure(figsize=(10, 4))\n",
        "\n",
        "# Plot the image\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(sample_image, cmap='gray')\n",
        "plt.title(f\"True label: {true_label}\")\n",
        "plt.axis('off')\n",
        "\n",
        "# Plot the probability distribution\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.bar(range(10), predictions[0])\n",
        "plt.xticks(range(10))\n",
        "plt.xlabel('Digit')\n",
        "plt.ylabel('Probability')\n",
        "plt.title('Softmax Probabilities')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "-cqgO3AlpwfJ",
        "outputId": "81175a6b-cec4-4ac1-d7e6-646d9425b662"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Softmax Probabilities')"
            ]
          },
          "metadata": {},
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x400 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAwQAAAGJCAYAAAA0SvTNAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAPQhJREFUeJzt3XtUVPXex/HPgHLxBiYKQgheUjQvKCihmVokeSu7mJkpYtkpwTS6KF1E81HUsjymeSkvR8uTHVPrZGmGoqslpunBrLyXl2OCWiqKCgr7+cPHeRxBG4yZAfb7tdas5fz47v37Diqbz/z23mMxDMMQAAAAAFNyc3UDAAAAAFyHQAAAAACYGIEAAAAAMDECAQAAAGBiBAIAAADAxAgEAAAAgIkRCAAAAAATIxAAAAAAJkYgAAAAAEyMQACn6dy5szp37lzi7Q4cOCCLxaK33nqr1HpJT0+XxWJRenp6qe0TACqiN998Uw0aNJC7u7vCw8Nd3U650rlzZzVv3rzU9leS4+GYMWNksVhsxkJDQzVo0CDr85IcC6/MvWDBghJ2jfKAQFABWCwWux788us49957rywWixITE13dCgAT27Fjhx555BGFhITIy8tLQUFBuvfee/Xuu+/e1P6+/vprvfzyy+rQoYPmz5+vCRMm6LffftOYMWOUmZlZus07yZVfbK883N3dVa9ePT344IPl9jWVpsWLF2vq1KmubgNOVsnVDeCvW7Rokc3zhQsXas2aNUXGmzZt6sy2TGPZsmXKyMhwdRsATG7jxo3q0qWL6tWrpyFDhiggIECHDx/Wpk2b9Pe//13Dhg0r8T7Xrl0rNzc3zZ07Vx4eHpKk77//XmPHjlVoaGi5XjHo16+funfvroKCAu3cuVMzZ87UV199pU2bNpXr13XFa6+9plGjRt2w5q677tL58+etf7fS5UDw448/asSIETa1ISEhOn/+vCpXruyIduFiBIIK4IknnrB5vmnTJq1Zs6bI+LXOnTunKlWqOLK1Cu/ChQt64YUXNHLkSI0ePdrV7QAwsfHjx8vHx0dbtmyRr6+vzdeOHTt2U/s8duyYvL29bX5hrCjatGljc5zs0KGD7r//fs2cOVOzZ88udpvc3FxVrVrVWS3+JZUqVVKlSjf+Nc/NzU1eXl527c9isdhdi/KHU4ZM4sp5jFu3btVdd92lKlWq6JVXXpF0+T/5mDFjimxz7bmGknTq1CmNGDFCwcHB8vT0VKNGjTRp0iQVFhaWuKf8/HyNHj1aERER8vHxUdWqVdWxY0etW7fuutu88847CgkJkbe3tzp16qQff/yxSM2uXbv0yCOP6JZbbpGXl5ciIyP1+eef/2k/586d065du3TixAm7X8PkyZNVWFioF1980e5tAMAR9u/fr9tvv71IGJCkOnXq2Dy/dOmSxo0bp4YNG8rT01OhoaF65ZVXlJeXZ62xWCyaP3++cnNzrafXLFiwQG3btpUkxcfH24xL/3+s+eGHH9SpUydVqVJFjRo10tKlSyVJ69evV1RUlLy9vdWkSRN98803Nn0dPHhQQ4cOVZMmTeTt7a1atWqpT58+OnDggLXGMAx16dJFtWvXtgk6+fn5atGihRo2bKjc3NwSf//uvvtuSdKvv/4qSVqwYIEsFovWr1+voUOHqk6dOrr11lut9e+9955uv/12eXp6KjAwUAkJCTp16lSx+966davat28vb29v1a9fX7NmzbL5uiOOh8VdQ3Cta68h6Ny5s1auXKmDBw9a/25DQ0MlXf8aAnuOuRcvXtTYsWN12223ycvLS7Vq1dKdd96pNWvW3LA/OA+BwER+//13devWTeHh4Zo6daq6dOlSou3PnTunTp066cMPP9TAgQM1bdo0dejQQcnJyUpKSipxPzk5Ofrggw/UuXNnTZo0SWPGjNHx48cVGxtb7HmcCxcu1LRp05SQkKDk5GT9+OOPuvvuu5WdnW2t+emnn3THHXdo586dGjVqlKZMmaKqVauqd+/eWr58+Q372bx5s5o2barp06fb1f+hQ4c0ceJETZo0Sd7e3iV67QBQ2kJCQrR169Zi3yi51lNPPaXRo0erTZs2euedd9SpUyelpqbqscces9YsWrRIHTt2lKenpxYtWqRFixapadOmeuONNyRJTz/9tHX8rrvusm538uRJ9ezZU1FRUZo8ebI8PT312GOPacmSJXrsscfUvXt3TZw4Ubm5uXrkkUd05swZ67ZbtmzRxo0b9dhjj2natGl65plnlJaWps6dO+vcuXOSLgeVefPm6cKFC3rmmWes26akpOinn37S/Pnzb+pd/P3790uSatWqZTM+dOhQ/fzzzxo9erT1FJwxY8YoISFBgYGBmjJlih5++GHNnj1bXbt21cWLF222P3nypLp3766IiAhNnjxZt956q5599lnNmzfPWuOI4+HNePXVVxUeHi4/Pz/r3+2Nriew95g7ZswYjR07Vl26dNH06dP16quvql69etq2bdtf6helyECFk5CQYFz7V9upUydDkjFr1qwi9ZKMlJSUIuMhISFGXFyc9fm4ceOMqlWrGnv27LGpGzVqlOHu7m4cOnTohn116tTJ6NSpk/X5pUuXjLy8PJuakydPGv7+/sbgwYOtY7/++qshyfD29jb++9//Wse/++47Q5Lx/PPPW8fuueceo0WLFsaFCxesY4WFhUb79u2N2267zTq2bt06Q5Kxbt26ImPFfS+K88gjjxjt27e3PpdkJCQk2LUtAJS2r7/+2nB3dzfc3d2N6Oho4+WXXzZWr15t5Ofn29RlZmYakoynnnrKZvzFF180JBlr1661jsXFxRlVq1a1qduyZYshyZg/f36RHq4caxYvXmwd27VrlyHJcHNzMzZt2mQdX716dZH9nDt3rsg+MzIyDEnGwoULbcZnz55tSDI+/PBDY9OmTYa7u7sxYsSI63+D/s+VY8rYsWON48ePG1lZWUZ6errRunVrQ5Lx6aefGoZhGPPnzzckGXfeeadx6dIl6/bHjh0zPDw8jK5duxoFBQXW8enTpxuSjHnz5hX5fkyZMsU6lpeXZ4SHhxt16tSx/t044niYkpJS5HeBa4/rxR0Le/ToYYSEhFz3+3b135e9x9xWrVoZPXr0KLJPlB2sEJiIp6en4uPjb3r7f/3rX+rYsaNq1qypEydOWB8xMTEqKCjQhg0bSrQ/d3d363mphYWF+uOPP3Tp0iVFRkYW+65B7969FRQUZH3erl07RUVF6csvv5Qk/fHHH1q7dq0effRRnTlzxtrf77//rtjYWO3du1dHjhy5bj+dO3eWYRjFnj51rXXr1unTTz/lTgwAyox7771XGRkZuv/++7V9+3ZNnjxZsbGxCgoKsjmF48rPzGtXdl944QVJ0sqVK/9SH9WqVbNZaWjSpIl8fX3VtGlTRUVFWcev/PmXX36xjl292nrx4kX9/vvvatSokXx9fYscF55++mnFxsZq2LBhGjBggBo2bKgJEybY3WdKSopq166tgIAAde7cWfv379ekSZP00EMP2dQNGTJE7u7u1ufffPON8vPzNWLECLm5udnU1ahRo8j3r1KlSvrb3/5mfe7h4aG//e1vOnbsmLZu3Sqp9I+HzlCSY66vr69++ukn7d2712n9oWS4qNhEgoKC/tKFYXv37tUPP/yg2rVrF/v1m7lo7R//+IemTJmiXbt22Syz1q9fv0jtbbfdVmSscePG+uSTTyRJ+/btk2EYev311/X6669ft8erf4jejEuXLum5557TgAEDrOfSAkBZ0LZtWy1btkz5+fnavn27li9frnfeeUePPPKIMjMz1axZMx08eFBubm5q1KiRzbYBAQHy9fXVwYMH/1IPt956a5Fz1318fBQcHFxkTLp8Ss0V58+fV2pqqubPn68jR47IMAzr106fPl1krrlz56phw4bau3evNm7cWKLTN59++mn16dNHbm5u8vX1tV4PcK1rj0dXvj9NmjSxGffw8FCDBg2KfP8CAwOLnMLUuHFjSZfPy7/jjjskle7x0BlKcsx944039MADD6hx48Zq3ry57rvvPg0YMEAtW7Z0Wr+4MQKBiZT0PPeCggKb54WFhbr33nv18ssvF1t/5QecvT788EMNGjRIvXv31ksvvaQ6derI3d1dqamp1nM5S+LKhc0vvviiYmNji6259gB4MxYuXKjdu3dr9uzZNhe6SdKZM2d04MAB1alThzs4AXAZDw8PtW3bVm3btlXjxo0VHx+vf/3rX0pJSbHW/NkFpzfr6nfT7Rm/+pf+YcOGaf78+RoxYoSio6Pl4+Mji8Wixx57rNibV6Snp1svhN6xY4eio6Pt7vO2225TTEzMn9Y54xqx0j4eOkNJjrl33XWX9u/fr88++0xff/21PvjgA73zzjuaNWuWnnrqKaf1jOsjEEA1a9YscmeE/Px8HT161GasYcOGOnv2rF0/QO2xdOlSNWjQQMuWLbM5MF19wLpacUuNe/bssd4BoUGDBpKkypUrl1qPxTl06JAuXryoDh06FPnawoULtXDhQi1fvly9e/d2WA8AYK/IyEhJsv5MDwkJUWFhofbu3Wvz+TTZ2dk6deqUQkJCbrg/RwUJ6fJxIS4uTlOmTLGOXbhwodi79xw9elTDhg1T165d5eHhYf3F9M/6/6uu7H/37t3W4450+bj566+/Fjn+/Pbbb0VuV7pnzx5Jsh6/Svt4+FfY+/db0mPuLbfcovj4eMXHx+vs2bO66667NGbMGAJBGcE1BFDDhg2LnP8/Z86cIisEjz76qDIyMrR69eoi+zh16pQuXbpUonmvvFt09btD33333XU/5GvFihU21wBs3rxZ3333nbp16ybp8m31OnfurNmzZxcJM5J0/PjxG/Zj721HH3vsMS1fvrzIQ5K6d++u5cuX25wnCwDOsG7dOpufp1dcOa/8yiku3bt3l6Qi10C9/fbbkqQePXrccJ4rv9he7xabf4W7u3uR1/Duu+8WOR5Jl8/ZLyws1Ny5czVnzhxVqlRJTz75ZLHfg9IUExMjDw8PTZs2zWauuXPn6vTp00W+f5cuXbL5XIP8/HzNnj1btWvXVkREhKTSPx7+FVWrVi329KxrleSY+/vvv9t8rVq1amrUqJHNbW7hWqwQQE899ZSeeeYZPfzww7r33nu1fft2rV69Wn5+fjZ1L730kj7//HP17NlTgwYNUkREhHJzc7Vjxw4tXbpUBw4cKLLNjfTs2VPLli3Tgw8+qB49eujXX3/VrFmz1KxZM509e7ZIfaNGjXTnnXfq2WefVV5enqZOnapatWrZnMI0Y8YM3XnnnWrRooWGDBmiBg0aKDs7WxkZGfrvf/+r7du3X7efzZs3q0uXLkpJSbnhhcVhYWEKCwsr9mv169dnZQCASwwbNkznzp3Tgw8+qLCwMOXn52vjxo1asmSJQkNDrTeVaNWqleLi4jRnzhydOnVKnTp10ubNm/WPf/xDvXv3/tNbUjds2FC+vr6aNWuWqlevrqpVqyoqKqrYc91LqmfPnlq0aJF8fHzUrFkzZWRk6JtvvilyK9D58+dr5cqVWrBggfWzAd5991098cQTmjlzpoYOHfqXe7me2rVrKzk5WWPHjtV9992n+++/X7t379Z7772ntm3bFvlQ0MDAQE2aNEkHDhxQ48aNtWTJEmVmZmrOnDnWT/11xPHwZkVERGjJkiVKSkpS27ZtVa1aNfXq1avYWnuPuc2aNVPnzp0VERGhW265Rd9//72WLl2qxMTEv9wvSomL7m4EB7rebUdvv/32YusLCgqMkSNHGn5+fkaVKlWM2NhYY9++fUVuT2YYhnHmzBkjOTnZaNSokeHh4WH4+fkZ7du3N956660it7a71rW3HS0sLDQmTJhghISEGJ6enkbr1q2NL774woiLi7O55dmVW529+eabxpQpU4zg4GDD09PT6Nixo7F9+/Yi8+zfv98YOHCgERAQYFSuXNkICgoyevbsaSxdutRaUxq3Hb2WuO0oABf66quvjMGDBxthYWFGtWrVDA8PD6NRo0bGsGHDjOzsbJvaixcvGmPHjjXq169vVK5c2QgODjaSk5Ntbh9pGMXfdtQwDOOzzz4zmjVrZlSqVMnmVpTXO9aEhIQUe9vJa39unjx50oiPjzf8/PyMatWqGbGxscauXbtsjkeHDx82fHx8jF69ehXZ34MPPmhUrVrV+OWXX677fbr6mHIjV247umXLlmK/Pn36dCMsLMyoXLmy4e/vbzz77LPGyZMnbWqufD++//57Izo62vDy8jJCQkKM6dOn29Q54nh4s7cdPXv2rPH4448bvr6+hiTr/MXddtQw7Dvm/s///I/Rrl07w9fX1/D29jbCwsKM8ePH/+nvDXAei2E4eG0NAAAAQJnFNQQAAACAiREIAAAAABMjEAAAAAAmRiAAAAAATIxAAAAAAJgYgQAAAAAwMT6YDADgUIWFhfrtt99UvXp1WSwWV7cDAKZhGIbOnDmjwMBAubldfx3A7kDAD3EAFQkfweI8v/32m4KDg13dBgCY1uHDh62f6l0cVggAAA5VvXp1SZcPSDVq1HBxNwBgHjk5OQoODrb+HL4eAgEAwKGurDDXqFGDQAAALvBnZ/pwUTEAAABgYgQCAAAAwMQIBAAAAICJEQgAAAAAEyMQAAAAACZGIAAAAABMjEAAAAAAmBiBAABMYsOGDerVq5cCAwNlsVi0YsWKP90mPT1dbdq0kaenpxo1aqQFCxY4vE8AgHMRCADAJHJzc9WqVSvNmDHDrvpff/1VPXr0UJcuXZSZmakRI0boqaee0urVqx3cKQDAmfikYgAwiW7duqlbt25218+aNUv169fXlClTJElNmzbVt99+q3feeUexsbGOahMA4GSsEAAAipWRkaGYmBibsdjYWGVkZNxwu7y8POXk5Ng8AABlFysEAIBiZWVlyd/f32bM399fOTk5On/+vLy9vYvdLjU1VWPHjnVGizCZ0FErnTLPgYk9nDIPUFawQgAAKFXJyck6ffq09XH48GFXtwQAuAFWCAAAxQoICFB2drbNWHZ2tmrUqHHd1QFJ8vT0lKenp6PbAwCUElYIAADFio6OVlpams3YmjVrFB0d7aKOAACOQCAAAJM4e/asMjMzlZmZKenybUUzMzN16NAhSZdP9Rk4cKC1/plnntEvv/yil19+Wbt27dJ7772nTz75RM8//7wr2gcAOAiBAABM4vvvv1fr1q3VunVrSVJSUpJat26t0aNHS5KOHj1qDQeSVL9+fa1cuVJr1qxRq1atNGXKFH3wwQfcchQAKhiLYRiGXYUWi6N7AQCnsfNHH0pBTk6OfHx8dPr0adWoUcPV7aAc4y5DQMnY+/OXFQIAAADAxAgEAAAAgIkRCAAAAAATIxAAAAAAJkYgAAAAAEyMQAAAAACYGIEAAAAAMDECAQAAAGBiBAIAAADAxAgEAAAAgIkRCAAAAAATIxAAAAAAJkYgAAAAAEyMQAAAAACYGIEAAAAAMDECAQAAAGBiBAIAAADAxAgEAAAAgIkRCAAAAAATIxAAAAAAJkYgAAAAAEyMQAAAAACYGIEAAAAAMDECAQAAAGBiBAIAAADAxAgEAAAAgIkRCAAAAAATIxAAAAAAJkYgAAAAAEyMQAAAAACYGIEAAAAAMDECAQAAAGBiBAIAAADAxAgEAAAAgIkRCAAAAAATIxAAAAAAJkYgAAAAAEyMQAAAAACYGIEAAAAAMDECAQAAAGBiBAIAAADAxAgEAAAAgIkRCAAAAAATIxAAAAAAJkYgAAAAAEyskqsbwGXJycl2106YMMHu2sWLF9td279/f7trK7quXbvaXbtq1Sq7a1euXGl3ba9eveyuBQAAuFmsEAAAAAAmRiAAAAAATIxAAAAmM2PGDIWGhsrLy0tRUVHavHnzDeunTp2qJk2ayNvbW8HBwXr++ed14cIFJ3ULAHA0AgEAmMiSJUuUlJSklJQUbdu2Ta1atVJsbKyOHTtWbP3ixYs1atQopaSkaOfOnZo7d66WLFmiV155xcmdAwAchUAAACby9ttva8iQIYqPj1ezZs00a9YsValSRfPmzSu2fuPGjerQoYMef/xxhYaGqmvXrurXr9+frioAAMoPAgEAmER+fr62bt2qmJgY65ibm5tiYmKUkZFR7Dbt27fX1q1brQHgl19+0Zdffqnu3btfd568vDzl5OTYPAAAZRe3HQUAkzhx4oQKCgrk7+9vM+7v769du3YVu83jjz+uEydO6M4775RhGLp06ZKeeeaZG54ylJqaqrFjx5Zq7wAAx2GFAABwXenp6ZowYYLee+89bdu2TcuWLdPKlSs1bty4626TnJys06dPWx+HDx92YscAgJJihQAATMLPz0/u7u7Kzs62Gc/OzlZAQECx27z++usaMGCAnnrqKUlSixYtlJubq6efflqvvvqq3NyKvq/k6ekpT0/P0n8BAACHYIUAAEzCw8NDERERSktLs44VFhYqLS1N0dHRxW5z7ty5Ir/0u7u7S5IMw3BcswAAp2GFoIyoUqWK3bUlOQifPXv2ZtoxvYYNGzpkv127drW7tk2bNnbXbtu27WbagQklJSUpLi5OkZGRateunaZOnarc3FzFx8dLkgYOHKigoCClpqZKknr16qW3335brVu3VlRUlPbt26fXX39dvXr1sgYDAED5RiAAABPp27evjh8/rtGjRysrK0vh4eFatWqV9ULjQ4cO2awIvPbaa7JYLHrttdd05MgR1a5dW7169dL48eNd9RIAAKWMQAAAJpOYmKjExMRiv5aenm7zvFKlSkpJSVFKSooTOgMAuALXEAAAAAAmRiAAAAAATIxAAAAAAJgYgQAAAAAwMQIBAAAAYGIEAgAAAMDECAQAAACAiREIAAAAABPjg8nKiD59+jhkv5mZmQ7Zb0XXsGFDh+z3/Pnzdtfm5OQ4pAcAAICrsUIAAAAAmBiBAAAAADAxAgEAAABgYgQCAAAAwMQIBAAAAICJEQgAAAAAEyMQAAAAACZGIAAAAABMjEAAAAAAmBiBAAAAADCxSq5uoCKrUaOG3bXe3t4O6eH48eMO2W951KdPH7trBwwY4JAejh49anftvn37HNIDAADA1VghAAAAAEyMQAAAAACYGIEAAAAAMDECAQAAAGBiBAIAAADAxAgEAAAAgIkRCAAAAAATIxAAAAAAJkYgAAAAAEyMQAAAAACYWCVXN1CRNW/e3O7a4OBgh/SwZ88eh+y3rPDy8rK7dsiQIXbX1q5d+2ba+VPnz593yH4BAABuFisEAAAAgIkRCAAAAAATIxAAAAAAJkYgAAAAAEyMQAAAAACYGIEAAAAAMDECAQAAAGBiBAIAAADAxAgEAAAAgIkRCACgHFi3bp2rWwAAVFCVXN0AHGvv3r2ubsGhJk+ebHdtTEyMAzuxzyeffOLqFlBO3Xfffbr11lsVHx+vuLg4BQcHu7olAEAFwQoBAJQDR44cUWJiopYuXaoGDRooNjZWn3zyifLz813dGgCgnCMQAEA54Ofnp+eff16ZmZn67rvv1LhxYw0dOlSBgYF67rnntH37dle3CAAopwgEAFDOtGnTRsnJyUpMTNTZs2c1b948RUREqGPHjvrpp59c3R4AoJwhEABAOXHx4kUtXbpU3bt3V0hIiFavXq3p06crOztb+/btU0hIiPr06ePqNgEA5QwXFQNAOTBs2DD985//lGEYGjBggCZPnqzmzZtbv161alW99dZbCgwMdGGXAIDyiEAAAOXAzz//rHfffVcPPfSQPD09i63x8/Pj9qQAgBLjlCEAKAdSUlLUp0+fImHg0qVL2rBhgySpUqVK6tSp05/ua8aMGQoNDZWXl5eioqK0efPmG9afOnVKCQkJqlu3rjw9PdW4cWN9+eWXN/9iAABlCoEAAMqBLl266I8//igyfvr0aXXp0sXu/SxZskRJSUlKSUnRtm3b1KpVK8XGxurYsWPF1ufn5+vee+/VgQMHtHTpUu3evVvvv/++goKCbvq1AADKFk4ZAoBywDAMWSyWIuO///67qlatavd+3n77bQ0ZMkTx8fGSpFmzZmnlypWaN2+eRo0aVaR+3rx5+uOPP7Rx40ZVrlxZkhQaGnpzLwIAUCYRCACgDHvooYckSRaLRYMGDbI5ZaigoEA//PCD2rdvb9e+8vPztXXrViUnJ1vH3NzcFBMTo4yMjGK3+fzzzxUdHa2EhAR99tlnql27th5//HGNHDlS7u7uxW6Tl5envLw86/OcnBy7+gMAuAaBwIGeeOIJV7dQ7qSkpJSo/tlnn3VQJ/Y7ffq03bXz5s1zYCeoiHx8fCRdXiGoXr26vL29rV/z8PDQHXfcoSFDhti1rxMnTqigoED+/v424/7+/tq1a1ex2/zyyy9au3at+vfvry+//FL79u3T0KFDdfHixev+f01NTdXYsWPt6gkA4HoEAgAow+bPny/p8mk6L774YolODyoNhYWFqlOnjubMmSN3d3dFREToyJEjevPNN68bCJKTk5WUlGR9npOTo+DgYGe1DAAoIQIBAJQDJV09K46fn5/c3d2VnZ1tM56dna2AgIBit6lbt64qV65sc3pQ06ZNlZWVpfz8fHl4eBTZxtPT87q3RgUAlD0EAgAoo9q0aaO0tDTVrFlTrVu3Lvai4iu2bdv2p/vz8PBQRESE0tLS1Lt3b0mXVwDS0tKUmJhY7DYdOnTQ4sWLVVhYKDe3yzem27Nnj+rWrVtsGAAAlD8EAgAoox544AHrO+1XfoH/q5KSkhQXF6fIyEi1a9dOU6dOVW5urvWuQwMHDlRQUJBSU1MlXb5OZ/r06Ro+fLiGDRumvXv3asKECXruuedKpR8AgOsRCACgjLr6NKHSOGVIkvr27avjx49r9OjRysrKUnh4uFatWmW90PjQoUPWlQBJCg4O1urVq/X888+rZcuWCgoK0vDhwzVy5MhS6QcA4HoEAgAwmcTExOueIpSenl5kLDo6Wps2bXJwVwAAVyEQAEAZVbNmzRteN3C14j7FGAAAexAIAKCMmjp1qqtbAACYAIEAAMqouLg4V7cAADABAgEAlFE5OTmqUaOG9c83cqUOAICSIhA40NUf5GNmTzzxhN21Jb1zSVn4Hm/cuNHu2mPHjjmwE1Q0NWvW1NGjR1WnTh35+voWez2BYRiyWCwqKChwQYcAgIqAQAAAZdTatWt1yy23SJLWrVvn4m4AABUVgQAAyqhOnToV+2cAAEoTgQAAyomTJ09q7ty52rlzpySpWbNmio+Pt64iAABwM9z+vAQA4GobNmxQaGiopk2bppMnT+rkyZOaNm2a6tevrw0bNri6PQBAOcYKAQCUAwkJCerbt69mzpxpvZi+oKBAQ4cOVUJCgnbs2OHiDgEA5RUrBABQDuzbt08vvPCCzZ213N3dlZSUpH379rmwMwBAeUcgAIByoE2bNtZrB662c+dOtWrVygUdAQAqCk4ZAoAy6ocffrD++bnnntPw4cO1b98+3XHHHZKkTZs2acaMGZo4caKrWgQAVAAEAgAoo8LDw2WxWGQYhnXs5ZdfLlL3+OOPq2/fvs5sDQBQgRAIAKCM+vXXX13dAgDABAgEDpSZmWl37ZkzZ+yurV69ut21ISEhdtfu2rXL7tqgoCC7a2fOnGl3rZeXl921ZcXBgwdd3QIqqJL8/wUA4GYRCACgHPn555916NAh5efn24zff//9LuoIAFDeEQgAoBz45Zdf9OCDD2rHjh021xVYLBZJlz+TAACAm8FtRwGgHBg+fLjq16+vY8eOqUqVKvrpp5+0YcMGRUZGKj093dXtAQDKMVYIAKAcyMjI0Nq1a+Xn5yc3Nze5ubnpzjvvVGpqqp577jn95z//cXWLAIByihUCACgHCgoKrDcU8PPz02+//Sbp8oXHu3fvdmVrAIByjhUCACgHmjdvru3bt6t+/fqKiorS5MmT5eHhoTlz5qhBgwaubg8AUI4RCACgHHjttdeUm5srSXrjjTfUs2dPdezYUbVq1dKSJUtc3B0AoDwjEABAORAbG2v9c6NGjbRr1y798ccfqlmzpvVOQwAA3AwCAQCUM4cPH5YkBQcHu7gTAEBFwEXFAFAOXLp0Sa+//rp8fHwUGhqq0NBQ+fj46LXXXtPFixdd3R4AoBxjhcCBZs6caXftHXfcYXftgAED7K4dO3as3bVr1qyxu/add96xu7Zq1ap215YVhYWFdteuWLHCcY0A/2fYsGFatmyZJk+erOjoaEmXb0U6ZswY/f777yX6eQMAwNUIBABQDixevFgff/yxunXrZh1r2bKlgoOD1a9fPwIBAOCmccoQAJQDnp6eCg0NLTJev359eXh4OL8hAECFQSAAgHIgMTFR48aNU15ennUsLy9P48ePV2Jiogs7AwCUd5wyBABl1EMPPWTz/JtvvtGtt96qVq1aSZK2b9+u/Px83XPPPa5oDwBQQRAIAKCM8vHxsXn+8MMP2zzntqMAgNJAIACAMmr+/PmubgEAYAIEAgAoR44fP67du3dLkpo0aaLatWu7uCMAQHnHRcUAUA7k5uZq8ODBqlu3ru666y7dddddCgwM1JNPPqlz5865uj0AQDlGIACAciApKUnr16/Xv//9b506dUqnTp3SZ599pvXr1+uFF15wdXsAgHKMU4YAoBz49NNPtXTpUnXu3Nk61r17d3l7e+vRRx/lg8kAADeNQFBGLFq0yO7aa+88ciOPPPKI3bV9+vSxu7Ykzp8/b3ft559/XqJ99+3bt6Tt2GXbtm1213799dcO6QG42rlz5+Tv719kvE6dOpwyBAD4SzhlCADKgejoaKWkpOjChQvWsfPnz2vs2LGKjo52YWcAgPKOFQIAKAemTp2q++67r8gHk3l5eWn16tUu7g4AUJ4RCACgHGjRooX27t2rjz76SLt27ZIk9evXT/3795e3t7eLuwMAlGcEAgAo4y5evKiwsDB98cUXGjJkiKvbAQBUMFxDAABlXOXKlW2uHQAAoDQRCACgHEhISNCkSZN06dIlV7cCAKhgOGUIAMqBLVu2KC0tTV9//bVatGihqlWr2nx92bJlLuoMAFDeEQgAoBzw9fXVww8/7Oo2AAAVEIEAAMqwwsJCvfnmm9qzZ4/y8/N19913a8yYMdxZCABQariGAADKsPHjx+uVV15RtWrVFBQUpGnTpikhIeEv7XPGjBkKDQ2Vl5eXoqKitHnzZru2+/jjj2WxWNS7d++/ND8AoGxhhaCM+OabbxxS++STT9pde//999tde/DgQbtr//73v9td26NHD7trJalv374lqrfXd99955D9AiW1cOFCvffee/rb3/4m6fL//x49euiDDz6Qm1vJ39NZsmSJkpKSNGvWLEVFRWnq1KmKjY3V7t27VadOnetud+DAAb344ovq2LHjTb8WAEDZxAoBAJRhhw4dUvfu3a3PY2JiZLFY9Ntvv93U/t5++20NGTJE8fHxatasmWbNmqUqVapo3rx5192moKBA/fv319ixY9WgQYObmhcAUHYRCACgDLt06ZK8vLxsxipXrqyLFy+WeF/5+fnaunWrYmJirGNubm6KiYlRRkbGdbd74403VKdOHbtXHPPy8pSTk2PzAACUXZwyBABlmGEYGjRokDw9Pa1jFy5c0DPPPGNz61F7bjt64sQJFRQUyN/f32bc399fu3btKnabb7/9VnPnzlVmZqbdPaempmrs2LF21wMAXItAAABlWFxcXJGxJ554wilznzlzRgMGDND7778vPz8/u7dLTk5WUlKS9XlOTo6Cg4Md0SIAoBQQCACgDJs/f36p7cvPz0/u7u7Kzs62Gc/OzlZAQECR+v379+vAgQPq1auXdaywsFCSVKlSJe3evVsNGzYssp2np6fNigYAoGzjGgIAMAkPDw9FREQoLS3NOlZYWKi0tDRFR0cXqQ8LC9OOHTuUmZlpfdx///3q0qWLMjMzedcfACoIVggAwESSkpIUFxenyMhItWvXTlOnTlVubq7i4+MlSQMHDlRQUJBSU1Pl5eWl5s2b22zv6+srSUXGAQDlF4EAAEykb9++On78uEaPHq2srCyFh4dr1apV1guNDx06dFOfbwAAKL8IBABgMomJiUpMTCz2a+np6TfcdsGCBaXfEADApXgbCAAAADAxVggquLlz5zqk1lEGDRrk6hYkSadOnXJ1CwAAAE7BCgEAAABgYgQCAAAAwMQIBAAAAICJEQgAAAAAEyMQAAAAACZGIAAAAABMjEAAAAAAmBiBAAAAADAxAgEAAABgYgQCAAAAwMQquboB4GpffPFFierDw8Ptrt2/f7/dtRMnTixRHwAAAOUVKwQAAACAiREIAAAAABMjEAAAAAAmRiAAAAAATIxAAAAAAJgYgQAAAAAwMQIBAAAAYGIEAgAAAMDECAQAAACAiREIAAAAABOr5OoGgKs1b97cYfs+f/683bW5ubkO6wMAAKAsYYUAAAAAMDECAQAAAGBiBAIAAADAxAgEAAAAgIkRCAAAAAATIxAAAAAAJkYgAAAAAEyMQAAAAACYGIEAAAAAMDECAQAAAGBilVzdAHC133//3WH7/uSTTxy2bwAAgPKKFQIAAADAxAgEAAAAgIkRCAAAAAATIxAAAAAAJkYgAAAAAEyMQAAAAACYGIEAAAAAMDECAQAAAGBiBAIAAADAxAgEAAAAgIlVcnUDwNXq1avnsH2fP3/eYfsGAAAor1ghAAAAAEyMQAAAAACYGIEAAExmxowZCg0NlZeXl6KiorR58+br1r7//vvq2LGjatasqZo1ayomJuaG9QCA8odAAAAmsmTJEiUlJSklJUXbtm1Tq1atFBsbq2PHjhVbn56ern79+mndunXKyMhQcHCwunbtqiNHjji5cwCAoxAIAMBE3n77bQ0ZMkTx8fFq1qyZZs2apSpVqmjevHnF1n/00UcaOnSowsPDFRYWpg8++ECFhYVKS0tzcucAAEchEACASeTn52vr1q2KiYmxjrm5uSkmJkYZGRl27ePcuXO6ePGibrnlluvW5OXlKScnx+YBACi7CAQAYBInTpxQQUGB/P39bcb9/f2VlZVl1z5GjhypwMBAm1BxrdTUVPn4+FgfwcHBf6lvAIBjEQgAAHaZOHGiPv74Yy1fvlxeXl7XrUtOTtbp06etj8OHDzuxSwBASfHBZABgEn5+fnJ3d1d2drbNeHZ2tgICAm647VtvvaWJEyfqm2++UcuWLW9Y6+npKU9Pz7/cLwDAOVghAACT8PDwUEREhM0FwVcuEI6Ojr7udpMnT9a4ceO0atUqRUZGOqNVAIATsUKAMqV27dqubgGo0JKSkhQXF6fIyEi1a9dOU6dOVW5uruLj4yVJAwcOVFBQkFJTUyVJkyZN0ujRo7V48WKFhoZarzWoVq2aqlWr5rLXAQAoPQQCADCRvn376vjx4xo9erSysrIUHh6uVatWWS80PnTokNzc/n/xeObMmcrPz9cjjzxis5+UlBSNGTPGma0DAByEQAAAJpOYmKjExMRiv5aenm7z/MCBA45vCADgUlxDAAAAAJgYgQAAAAAwMQIBAAAAYGIEAgAAAMDECAQAAACAiREIAAAAABMjEAAAAAAmRiAAAAAATIwPJkOZcvbsWVe3AAAAYCqsEAAAAAAmRiAAAAAATIxAAAAAAJgYgQAAAAAwMQIBAAAAYGIEAgAAAMDECAQAAACAiREIAAAAABMjEAAAAAAmRiAAAAAATKySqxsArtavX78S1X/00UcO6gQAAMAcWCEAAAAATIxAAAAAAJgYgQAAAAAwMQIBAAAAYGIEAgAAAMDECAQAAACAiREIAAAAABMjEAAAAAAmRiAAAAAATIxAAAAAAJhYJVc3AFztyJEjJarv3LmzYxoBAAAwCVYIAAAAABMjEAAAAAAmRiAAAAAATIxAAAAAAJgYgQAAAAAwMQIBAAAAYGIEAgAAAMDECAQAAACAiREIAAAAABMjEAAAAAAmRiAAAAAATIxAAAAAAJgYgQAAAAAwMQIBAAAAYGIEAgAAAMDECAQAAACAiREIAAAAABMjEACAycyYMUOhoaHy8vJSVFSUNm/efMP6f/3rXwoLC5OXl5datGihL7/80kmdAgCcgUAAACayZMkSJSUlKSUlRdu2bVOrVq0UGxurY8eOFVu/ceNG9evXT08++aT+85//qHfv3urdu7d+/PFHJ3cOAHAUi2EYhl2FFoujewEAp7HzR1+FExUVpbZt22r69OmSpMLCQgUHB2vYsGEaNWpUkfq+ffsqNzdXX3zxhXXsjjvuUHh4uGbNmmXXnDk5OfLx8dHp06dVo0aN0nkhMKXQUSudMs+BiT2cMg/gaPb+/K3kxJ4AAC6Un5+vrVu3Kjk52Trm5uammJgYZWRkFLtNRkaGkpKSbMZiY2O1YsWK686Tl5envLw86/PTp09LunxgAv6KwrxzTpmHf6uoKK78W/6zN8HsDgRmfTcNACqKEydOqKCgQP7+/jbj/v7+2rVrV7HbZGVlFVuflZV13XlSU1M1duzYIuPBwcE30TXgfD5TXd0BULrOnDkjHx+f636dFQIAQKlKTk62WVUoLCzUH3/8oVq1ajnt9NOcnBwFBwfr8OHDLjlNydXz00PZmJ8eysb8Zu7BMAydOXNGgYGBN6wjEACASfj5+cnd3V3Z2dk249nZ2QoICCh2m4CAgBLVS5Knp6c8PT1txnx9fW+u6b+oRo0aLr1uwdXz00PZmJ8eysb8Zu3hRisDV3CXIQAwCQ8PD0VERCgtLc06VlhYqLS0NEVHRxe7TXR0tE29JK1Zs+a69QCA8ocVAgAwkaSkJMXFxSkyMlLt2rXT1KlTlZubq/j4eEnSwIEDFRQUpNTUVEnS8OHD1alTJ02ZMkU9evTQxx9/rO+//15z5sxx5csAAJQiAgEAmEjfvn11/PhxjR49WllZWQoPD9eqVausFw4fOnRIbm7/v3jcvn17LV68WK+99ppeeeUV3XbbbVqxYoWaN2/uqpdgF09PT6WkpBQ5dcks89ND2ZifHsrG/PTw5+z+HAIAAAAAFQ/XEAAAAAAmRiAAAAAATIxAAAAAAJgYgQAAAAAwMQIBAKBCmTFjhkJDQ+Xl5aWoqCht3rzZaXNv2LBBvXr1UmBgoCwWi1asWOG0ua9ITU1V27ZtVb16ddWpU0e9e/fW7t27nTb/zJkz1bJlS+uHL0VHR+urr75y2vzFmThxoiwWi0aMGOG0OceMGSOLxWLzCAsLc9r8knTkyBE98cQTqlWrlry9vdWiRQt9//33Tps/NDS0yPfAYrEoISHBaT0UFBTo9ddfV/369eXt7a2GDRtq3LhxcuY9dc6cOaMRI0YoJCRE3t7eat++vbZs2eK0+e1BIAAAVBhLlixRUlKSUlJStG3bNrVq1UqxsbE6duyYU+bPzc1Vq1atNGPGDKfMV5z169crISFBmzZt0po1a3Tx4kV17dpVubm5Tpn/1ltv1cSJE7V161Z9//33uvvuu/XAAw/op59+csr819qyZYtmz56tli1bOn3u22+/XUePHrU+vv32W6fNffLkSXXo0EGVK1fWV199pZ9//llTpkxRzZo1ndbDli1bbF7/mjVrJEl9+vRxWg+TJk3SzJkzNX36dO3cuVOTJk3S5MmT9e677zqth6eeekpr1qzRokWLtGPHDnXt2lUxMTE6cuSI03r4UwYAABVEu3btjISEBOvzgoICIzAw0EhNTXV6L5KM5cuXO33eax07dsyQZKxfv95lPdSsWdP44IMPnD7vmTNnjNtuu81Ys2aN0alTJ2P48OFOmzslJcVo1aqV0+a71siRI40777zTZfMXZ/jw4UbDhg2NwsJCp83Zo0cPY/DgwTZjDz30kNG/f3+nzH/u3DnD3d3d+OKLL2zG27RpY7z66qtO6cEerBAAACqE/Px8bd26VTExMdYxNzc3xcTEKCMjw4Wdudbp06clSbfccovT5y4oKNDHH3+s3NxcRUdHO33+hIQE9ejRw+bfhDPt3btXgYGBatCggfr3769Dhw45be7PP/9ckZGR6tOnj+rUqaPWrVvr/fffd9r818rPz9eHH36owYMHy2KxOG3e9u3bKy0tTXv27JEkbd++Xd9++626devmlPkvXbqkgoICeXl52Yx7e3s7dcXoz/BJxQCACuHEiRMqKCiwfuryFf7+/tq1a5eLunKtwsJCjRgxQh06dHDqp0vv2LFD0dHRunDhgqpVq6bly5erWbNmTptfkj7++GNt27bNZedqR0VFacGCBWrSpImOHj2qsWPHqmPHjvrxxx9VvXp1h8//yy+/aObMmUpKStIrr7yiLVu26LnnnpOHh4fi4uIcPv+1VqxYoVOnTmnQoEFOnXfUqFHKyclRWFiY3N3dVVBQoPHjx6t///5Omb969eqKjo7WuHHj1LRpU/n7++uf//ynMjIy1KhRI6f0YA8CAQAAFVRCQoJ+/PFHp78T2aRJE2VmZur06dNaunSp4uLitH79eqeFgsOHD2v48OFas2ZNkXdmneXqd6BbtmypqKgohYSE6JNPPtGTTz7p8PkLCwsVGRmpCRMmSJJat26tH3/8UbNmzXJJIJg7d666deumwMBAp877ySef6KOPPtLixYt1++23KzMzUyNGjFBgYKDTvg+LFi3S4MGDFRQUJHd3d7Vp00b9+vXT1q1bnTK/PQgEAIAKwc/PT+7u7srOzrYZz87OVkBAgIu6cp3ExER98cUX2rBhg2699Vanzu3h4WF99zMiIkJbtmzR3//+d82ePdsp82/dulXHjh1TmzZtrGMFBQXasGGDpk+frry8PLm7uzullyt8fX3VuHFj7du3zynz1a1bt0gAa9q0qT799FOnzH+1gwcP6ptvvtGyZcucPvdLL72kUaNG6bHHHpMktWjRQgcPHlRqaqrTAkHDhg21fv165ebmKicnR3Xr1lXfvn3VoEEDp8xvD64hAABUCB4eHoqIiFBaWpp1rLCwUGlpaS45f91VDMNQYmKili9frrVr16p+/fqubkmFhYXKy8tz2nz33HOPduzYoczMTOsjMjJS/fv3V2ZmptPDgCSdPXtW+/fvV926dZ0yX4cOHYrcbnbPnj0KCQlxyvxXmz9/vurUqaMePXo4fe5z587Jzc321113d3cVFhY6vZeqVauqbt26OnnypFavXq0HHnjA6T1cDysEAIAKIykpSXFxcYqMjFS7du00depU5ebmKj4+3inznz171uYd4F9//VWZmZm65ZZbVK9ePaf0kJCQoMWLF+uzzz5T9erVlZWVJUny8fGRt7e3w+dPTk5Wt27dVK9ePZ05c0aLFy9Wenq6Vq9e7fC5r6hevXqRayaqVq2qWrVqOe1aihdffFG9evVSSEiIfvvtN6WkpMjd3V39+vVzyvzPP/+82rdvrwkTJujRRx/V5s2bNWfOHM2ZM8cp819RWFio+fPnKy4uTpUqOf/Xzl69emn8+PGqV6+ebr/9dv3nP//R22+/rcGDBzuth9WrV8swDDVp0kT79u3TSy+9pLCwMKf9XLKLq29zBABAaXr33XeNevXqGR4eHka7du2MTZs2OW3udevWGZKKPOLi4pzWQ3HzSzLmz5/vlPkHDx5shISEGB4eHkbt2rWNe+65x/j666+dMveNOPu2o3379jXq1q1reHh4GEFBQUbfvn2Nffv2OW1+wzCMf//730bz5s0NT09PIywszJgzZ45T5zcMw1i9erUhydi9e7fT5zYMw8jJyTGGDx9u1KtXz/Dy8jIaNGhgvPrqq0ZeXp7TeliyZInRoEEDw8PDwwgICDASEhKMU6dOOW1+e1gMw4kf1QYAAACgTOEaAgAAAMDECAQAAACAiREIAAAAABMjEAAAAAAmRiAAAAAATIxAAAAAAJgYgQAAAAAwMQIBAAAAYGIEAgAAgP9jsVi0YsUKu+vT09NlsVh06tQph/UEOBqBAAAAVHiDBg2SxWKRxWJR5cqV5e/vr3vvvVfz5s1TYWGhte7o0aPq1q2b3ftt3769jh49Kh8fH0nSggUL5OvrW9rtAw5FIAAAAKZw33336ejRozpw4IC++uordenSRcOHD1fPnj116dIlSVJAQIA8PT3t3qeHh4cCAgJksVgc1TbgcAQCAABgCp6engoICFBQUJDatGmjV155RZ999pm++uorLViwQFLRU4Y2btyo8PBweXl5KTIyUitWrJDFYlFmZqYk21OG0tPTFR8fr9OnT1tXI8aMGeP01wmUFIEAAACY1t13361WrVpp2bJlRb6Wk5OjXr16qUWLFtq2bZvGjRunkSNHXndf7du319SpU1WjRg0dPXpUR48e1YsvvujI9oFSUcnVDQAAALhSWFiYfvjhhyLjixcvlsVi0fvvvy8vLy81a9ZMR44c0ZAhQ4rdj4eHh3x8fGSxWBQQEODotoFSwwoBAAAwNcMwir0GYPfu3WrZsqW8vLysY+3atXNma4BTEAgAAICp7dy5U/Xr13d1G4DLEAgAAIBprV27Vjt27NDDDz9c5GtNmjTRjh07lJeXZx3bsmXLDffn4eGhgoKCUu8TcCQCAQAAMIW8vDxlZWXpyJEj2rZtmyZMmKAHHnhAPXv21MCBA4vUP/744yosLNTTTz+tnTt3avXq1Xrrrbck6bq3GQ0NDdXZs2eVlpamEydO6Ny5cw59TUBpIBAAAABTWLVqlerWravQ0FDdd999WrdunaZNm6bPPvtM7u7uRepr1Kihf//738rMzFR4eLheffVVjR49WpJsriu4Wvv27fXMM8+ob9++ql27tiZPnuzQ1wSUBothGIarmwAAACgPPvroI+tnDXh7e7u6HaBUcNtRAACA61i4cKEaNGigoKAgbd++XSNHjtSjjz5KGECFQiAAAAC4jqysLI0ePVpZWVmqW7eu+vTpo/Hjx7u6LaBUccoQAAAAYGJcVAwAAACYGIEAAAAAMDECAQAAAGBiBAIAAADAxAgEAAAAgIkRCAAAAAATIxAAAAAAJkYgAAAAAEzsfwGP8PSSH4dVhQAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Demonstrate using a custom softmax function in TensorFlow\n",
        "def custom_softmax(logits):\n",
        "    \"\"\"Custom implementation of softmax in TensorFlow\"\"\"\n",
        "    exp_logits = tf.exp(logits - tf.reduce_max(logits, axis=-1, keepdims=True))\n",
        "    return exp_logits / tf.reduce_sum(exp_logits, axis=-1, keepdims=True)\n",
        "\n",
        "# Example usage of custom softmax\n",
        "logits = tf.constant([[2.0, 1.0, 0.5], [3.0, 2.0, 1.0]])\n",
        "custom_probs = custom_softmax(logits)\n",
        "tf_probs = tf.nn.softmax(logits)\n",
        "\n",
        "print(\"\\nCustom softmax:\", custom_probs.numpy())\n",
        "print(\"TensorFlow softmax:\", tf_probs.numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MtCPsE0Upz0w",
        "outputId": "81b14c5d-039e-4198-e9b4-52e1102d6be3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Custom softmax: [[0.6285317  0.2312239  0.14024438]\n",
            " [0.66524094 0.24472848 0.09003057]]\n",
            "TensorFlow softmax: [[0.6285317  0.2312239  0.14024438]\n",
            " [0.66524094 0.24472848 0.09003057]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This example compares a custom implementation of softmax with TensorFlow's built-in `tf.nn.softmax()` function. Both should produce identical results. The custom implementation also includes the numerical stability trick of subtracting the maximum value before exponentiation.\n",
        "\n"
      ],
      "metadata": {
        "id": "-mM8ztrZqVrq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modern NLP architectures like BERT, GPT, and T5 use softmax in multiple components:\n",
        "\n",
        "- In attention mechanisms to determine how much focus to place on different parts of the input\n",
        "- In the final classification layer for tasks like sentiment analysis or document classification\n",
        "\n",
        "Variational Autoencoders (VAEs) and certain Generative Adversarial Networks (GANs) use softmax in their discriminator components or for categorical latent variable modeling."
      ],
      "metadata": {
        "id": "aZJB-Z7WqdMd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Softmax vs Sigmoid Activation Function\n",
        "\n",
        "The sigmoid function, defined as $σ(x) = 1/(1+e^(-x))$, transforms a single real number into a value between 0 and 1. In contrast, the softmax activation function operates on a vector of numbers, converting them into a probability distribution.\n",
        "\n",
        "Here are the fundamental differences between these two activation functions:\n",
        "\n",
        "__Input and output dimensions__\n",
        "- Sigmoid: Takes a single scalar input and produces a single scalar output between 0 and 1.\n",
        "- Softmax: Takes a vector of n inputs and produces a vector of n outputs that sum to 1.\n",
        "\n",
        "__Probability interpretation__\n",
        "Sigmoid: Output represents the probability of a single binary event (e.g., \"Is this email spam?\").\n",
        "Softmax: Outputs represent probabilities across multiple mutually exclusive classes (e.g., \"Which digit from 0-9 is this?\").\n",
        "\n",
        "__Independence of outputs__\n",
        "- Sigmoid: When used across multiple neurons, each output is independent of others. Multiple outputs can all be high or all be low.\n",
        "- Softmax: Outputs are interdependent—if one probability increases, others must decrease to maintain a sum of 1.\n",
        "\n",
        "\n",
        "## Mathematical relationship\n",
        "\n",
        "Interestingly, the sigmoid function is actually a special case of the softmax activation function when there are only two classes. This is why sigmoid is often called \"binary softmax.\"\n",
        "\n",
        "\n",
        "## When to use softmax vs when to use sigmoid\n",
        "\n",
        "Choosing the right activation function is crucial for ensuring a neural network outputs meaningful predictions, and the decision between softmax and sigmoid depends on the nature of the classification task.\n",
        "\n",
        "Use sigmoid when:\n",
        "\n",
        "- Solving binary classification problems (two classes)\n",
        "- Each output needs to be interpreted independently\n",
        "- Implementing multi-label classification where an instance can belong to multiple classes simultaneously\n",
        "- Creating attention mechanisms or gates in recurrent neural networks\n",
        "\n",
        "Use softmax when:\n",
        "\n",
        "- Dealing with multi-class classification problems (more than two classes)\n",
        "- Classes are mutually exclusive (an input belongs to exactly one class)\n",
        "- You need a proper probability distribution across all possible outcomes\n",
        "- Implementing the final layer of most image classification networks\n",
        "\n",
        "| Feature    | Sigmoid   | Softmax |\n",
        "| ---------- | --------------- | -------------- |\n",
        "|  Input | Single scalar | Vector of values  |\n",
        "|  Output range | Between 0 and 1 | Between 0 and 1, summing to 1  |\n",
        "|  Use case | Binary classification | Multi-class classification  |\n",
        "|  Output Interpretation | Independent probability | Probability distribution  |\n",
        "|  Multiple Outputs | Can all be high or can all be low | Constrained to sum to 1  |\n",
        "|  Gradients | Can suffer from vanishing gradient | Less prone to vanishing gradient issues  |\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Source: https://www.datacamp.com/tutorial/softmax-activation-function-in-python"
      ],
      "metadata": {
        "id": "Mltm7TQcqmWN"
      }
    }
  ]
}