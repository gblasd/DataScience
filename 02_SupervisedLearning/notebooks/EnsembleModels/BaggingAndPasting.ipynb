{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8522f03",
   "metadata": {},
   "source": [
    "# Bagging and Pasting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858a30c8",
   "metadata": {},
   "source": [
    "One way to get a diverse set of classifiers is to use very different training algorithms. Another approach is to use the same training algorithm for every predictor, but to train on different random subsets of the training set. When sampling is performed _with_ replacement, this method is called __bagging__ (_boostrap aggregating_). When sampling is performed _without_ replacement, it is called __pasting__. \n",
    "\n",
    "In other words, both bagginf and pasting allow training instances to be sampled several times across multiple predictors, but only bagging allows training instances to be sampled several times for the same predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c218734d",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "\n",
    "flowchart TD\n",
    "    A[Training set] -->|Random sampling| DATA_A\n",
    "    A[Training set] -->|Random sampling| DATA_B\n",
    "    A[Training set] -->|Random sampling| DATA_C\n",
    "    A[Training set] -->|Random sampling| DATA_D\n",
    "  \n",
    "    DATA_A-->|Training| B[Predictors]\n",
    "    DATA_B-->|Training| C[Predictors]\n",
    "    DATA_C-->|Training| D[Predictors]\n",
    "    DATA_D-->|Training| E[Predictors]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bec8bb8",
   "metadata": {},
   "source": [
    "Once all predictors are trained, the ensemble can make a prediction for a new instance by simply aggregating the predictions of all predictors. The aggregation function is typically the _statistical mode_ (i.e. the most frequent prediction, just like a hard voting classifier) for classification, or the average for regression. Each infividual predictor has a higher a bias than if it were trained on the original training set, but aggregation reduces both bias and variance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d54129eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bd1ef02",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = iris[\"data\"][:,(2,3)] # petal length, petal width\n",
    "y = (iris[\"target\"] == 2).astype(np.float64) # Iris-Virginica\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee0f5ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(),\n",
    "    n_estimators=500,\n",
    "    max_samples=100, \n",
    "    bootstrap=True,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "bag_clf.fit(X_train, y_train)\n",
    "y_pred = bag_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f923566",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9736842105263158"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
